# ğŸŒ Synapse E2E Project with Spark Pool

This project demonstrates an **end-to-end (E2E) data engineering pipeline** in **Azure Synapse Analytics (Spark Pool)**.  
The pipeline ingests raw earthquake data from the **USGS Earthquake API**, processes it through **Bronze â†’ Silver â†’ Gold layers**, and outputs curated datasets for analytics and reporting.

---

âœ… This README has:  
- **Modern collapsible repo structure**  
- **Workflow diagram (Mermaid)**  
- **Detailed Bronze/Silver/Gold sections** with functions explained  
- **Hyperlinked images** (so screenshots show up directly in GitHub preview)  
- **End-to-End pipeline flow**  

---


## ğŸ“‚ Repository Structure

<details>
<summary>ğŸ“‚ Root</summary>

- ğŸ–¼ï¸ [Cancelling a job before executing another](./Cancelling%20a%20job%20before%20executing%20another.png)  
- ğŸ–¼ï¸ [Execute all notebooks with a pipeline](./Execute%20all%20notebooks%20with%20a%20pipeline.png)  
- ğŸ–¼ï¸ [Extracted JSON data](./Extracted%20Json%20data.png)  
- ğŸ“œ [combined_notebook.py](./combined_notebook.py)  

</details>

<details>
<summary>ğŸ“‚ Bronze</summary>

- ğŸ““ [Bronze Notebook.ipynb](./Bronze/Bronze%20Notebook.ipynb)  
- ğŸ–¼ï¸ [Bronze_notebook Spark execution](./Bronze/Bronze_notebook%20Spark%20execution.png)  

</details>

<details>
<summary>ğŸ“‚ Silver</summary>

- ğŸ““ [Silver Notebook.ipynb](./Silver/Silver%20Notebook.ipynb)  

</details>

<details>
<summary>ğŸ“‚ Gold</summary>

- ğŸ““ [Gold Notebook.ipynb](./Gold/Gold%20Notebook.ipynb)  

</details>

---

## âš™ï¸ Workflow

```mermaid
graph TD
    A[ğŸŒ USGS Earthquake API] --> B[ğŸ¥‰ Bronze Layer: Raw Ingestion]
    B --> C[ğŸ¥ˆ Silver Layer: Data Cleaning & Transformation]
    C --> D[ğŸ¥‡ Gold Layer: Enrichment & Aggregation]
    D --> E[ğŸ“Š Analytics & Reporting]

## ğŸ¥‰ Bronze Layer (Raw Data Ingestion)

**Goal:** Ingest raw earthquake data (JSON) from the **USGS API** and store it in the Bronze layer (**ADLS Gen2**).

### ğŸ”¹ Key Steps
- Mount ADLS Gen2 and define storage paths.  
- Fetch data using Pythonâ€™s `requests` library.  
- Save API response to **ADLS Bronze container**.  
- Return metadata (`start_date`, ADLS paths) to pipeline using `mssparkutils.notebook.exit()`.  

### ğŸ”¹ Functions & Libraries Used
- `requests.get()` â†’ Fetch earthquake data from API  
- `json.dumps()` â†’ Serialize API response  
- `spark.read.json()` â†’ Load JSON into Spark DataFrame  
- `df.write.json()` â†’ Persist raw data in Bronze container  

ğŸ“· [Extracted JSON Data](./Extracted%20Json%20data.png)

---

## ğŸ¥ˆ Silver Layer (Data Transformation & Validation)

**Goal:** Transform raw Bronze data into structured Silver tables for easier querying and validation.  

### ğŸ”¹ Key Steps
- Flatten JSON fields into structured columns (longitude, latitude, elevation, title, place, magnitude, etc.).  
- Validate missing values using `when` and `isnull`.  
- Convert UNIX timestamps to proper `TimestampType`.  
- Save transformed dataset to **Parquet format** in Silver container.  

### ğŸ”¹ Functions & Libraries Used
- `col()` â†’ Select and transform JSON fields  
- `isnull()` & `when()` â†’ Handle missing/null values  
- `cast(TimestampType())` â†’ Convert time fields  
- `df.write.parquet()` â†’ Save structured Silver dataset  

---

## ğŸ¥‡ Gold Layer (Enrichment & Aggregation)

**Goal:** Enrich Silver data with geospatial attributes and classify events by significance for analytics.  

### ğŸ”¹ Key Steps
- Read from **Silver dataset**.  
- Enrich data with **country codes** using `reverse_geocoder`.  
- Classify earthquake significance (`sig_class`) as **Low, Moderate, High**.  
- Save enriched dataset to **Parquet format** in Gold container.  

### ğŸ”¹ Functions & Libraries Used
- `reverse_geocoder.search()` â†’ Get country codes from lat/lon  
- `udf()` â†’ Register custom Python UDF for Spark  
- `withColumn()` â†’ Add derived attributes  
- `when()` â†’ Create significance classification rules  
- `df.write.parquet()` â†’ Save curated Gold dataset  

---

## ğŸ“Š End-to-End Flow

1. **Bronze Notebook** â†’ API ingestion (raw data in JSON).  
2. **Silver Notebook** â†’ Data shaping, validation, timestamp conversion.  
3. **Gold Notebook** â†’ Enrichment (location, classification) for analytics.  
4. **Pipeline Orchestration** â†’ Synapse pipelines run notebooks in order.  

ğŸ“· [Execute all notebooks with a pipeline](./Execute%20all%20notebooks%20with%20a%20pipeline.png)

---

## ğŸš€ Key Learnings

- How to integrate **external APIs** with Synapse Spark Pools.  
- Ingest and store data in **ADLS Gen2** following **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).  
- Use **PySpark functions** for data validation, transformation, and enrichment.  
- Orchestrate notebooks via **Azure Synapse Pipelines** for automation.  

